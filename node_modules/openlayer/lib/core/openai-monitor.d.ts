import { RequestOptions } from 'openai/core';
import { ChatCompletion, ChatCompletionChunk, ChatCompletionCreateParams, Completion, CompletionCreateParams } from 'openai/resources';
import { Run } from 'openai/resources/beta/threads/runs/runs';
import { Stream } from 'openai/streaming';
import Openlayer from "../../index.js";
type OpenAIMonitorConstructorProps = {
    openAiApiKey: string;
    openlayerClient: Openlayer;
    openlayerInferencePipelineId: string;
};
/**
 * @deprecated The OpenAIMonitor is deprecated. Please use the OpenAI tracer module instead. Refer to https://www.openlayer.com/docs/monitoring/publishing-data#openai for details.
 */
declare class OpenAIMonitor {
    private openlayerClient;
    private openAIClient;
    private openlayerInferencePipelineId;
    private defaultConfig;
    /**
     * Constructs an OpenAIMonitor instance.
     * @param {OpenAIMonitorConstructorProps} props - The configuration properties for the OpenAI and Openlayer clients.
     */
    constructor({ openAiApiKey, openlayerClient, openlayerInferencePipelineId, }: OpenAIMonitorConstructorProps);
    private cost;
    private chatCompletionPrompt;
    private threadPrompt;
    private inputVariables;
    /**
     * Creates a chat completion using the OpenAI client and streams the result to Openlayer.
     * @param {ChatCompletionCreateParams} body - The parameters for creating a chat completion.
     * @param {RequestOptions} [options] - Optional request options.
     * @param {Openlayer.RequestOptions<any> | undefined} [additionalLogs] - Optional metadata logs to include with the request sent to Openlayer.
     * @returns {Promise<ChatCompletion | Stream<ChatCompletionChunk>>} Promise of a ChatCompletion or a Stream
     * @throws {Error} Throws errors from the OpenAI client.
     */
    createChatCompletion: (body: ChatCompletionCreateParams, options?: RequestOptions, additionalLogs?: Openlayer.RequestOptions | undefined) => Promise<ChatCompletion | Stream<ChatCompletionChunk>>;
    /**
     * Creates a completion using the OpenAI client and streams the result to Openlayer.
     * @param {CompletionCreateParams} body - The parameters for creating a completion.
     * @param {RequestOptions} [options] - Optional request options.
     * @param {Openlayer.RequestOptions<any> | undefined} [additionalLogs] - Optional metadata logs to include with the request sent to Openlayer.
     * @returns {Promise<Completion | Stream<Completion>>} Promise that resolves to a Completion or a Stream.
     * @throws {Error} Throws errors from the OpenAI client.
     */
    createCompletion: (body: CompletionCreateParams, options?: RequestOptions, additionalLogs?: Openlayer.RequestOptions | undefined) => Promise<Completion | Stream<Completion>>;
    /**
     * Monitor a run from an OpenAI assistant.
     * Once the run is completed, the thread data is published to Openlayer,
     * along with the latency, cost, and number of tokens used.
     * @param {Run} run - The run created by the OpenAI assistant.
     * @param {Openlayer.RequestOptions<any> | undefined} [additionalLogs] - Optional metadata logs to include with the request sent to Openlayer.
     * @returns {Promise<void>} A promise that resolves when the run data has been successfully published to Openlayer.
     */
    monitorThreadRun(run: Run, additionalLogs?: Openlayer.RequestOptions | undefined): Promise<void>;
}
export default OpenAIMonitor;
//# sourceMappingURL=openai-monitor.d.ts.map