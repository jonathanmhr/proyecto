"use strict";
Object.defineProperty(exports, "__esModule", { value: true });
exports.OpenlayerHandler = void 0;
const base_1 = require("@langchain/core/callbacks/base");
const messages_1 = require("@langchain/core/messages");
const tracer_1 = require("../tracing/tracer.js");
const LANGCHAIN_TO_OPENLAYER_PROVIDER_MAP = {
    openai: 'OpenAI',
    'openai-chat': 'OpenAI',
    'chat-ollama': 'Ollama',
    vertexai: 'Google',
};
const PROVIDER_TO_STEP_NAME = {
    OpenAI: 'OpenAI Chat Completion',
    Ollama: 'Ollama Chat Completion',
    Google: 'Google Vertex AI Chat Completion',
};
class OpenlayerHandler extends base_1.BaseCallbackHandler {
    constructor(kwargs = {}) {
        super();
        this.name = 'OpenlayerHandler';
        this.startTime = null;
        this.endTime = null;
        this.prompt = null;
        this.latency = 0;
        this.model = null;
        this.modelParameters = null;
        this.promptTokens = 0;
        this.completionTokens = 0;
        this.totalTokens = 0;
        this.output = '';
        this.metadata = kwargs;
    }
    async handleChatModelStart(llm, messages, runId, parentRunId, extraParams, tags, metadata, name) {
        this.initializeRun(extraParams || {}, metadata || {});
        this.prompt = this.langchainMassagesToPrompt(messages);
        this.startTime = performance.now();
    }
    initializeRun(extraParams, metadata) {
        this.modelParameters = extraParams['invocation_params'] || {};
        const provider = metadata?.['ls_provider'];
        if (provider && LANGCHAIN_TO_OPENLAYER_PROVIDER_MAP[provider]) {
            this.provider = LANGCHAIN_TO_OPENLAYER_PROVIDER_MAP[provider];
        }
        this.model = this.modelParameters?.['model'] || metadata['ls_model_name'] || null;
        this.output = '';
    }
    langchainMassagesToPrompt(messages) {
        let prompt = [];
        for (const message of messages) {
            for (const m of message) {
                if (m instanceof messages_1.AIMessage) {
                    prompt.push({ role: 'assistant', content: m.content });
                }
                else if (m instanceof messages_1.SystemMessage) {
                    prompt.push({ role: 'system', content: m.content });
                }
                else {
                    prompt.push({ role: 'user', content: m.content });
                }
            }
        }
        return prompt;
    }
    async handleLLMStart(llm, prompts, runId, parentRunId, extraParams, tags, metadata, runName) {
        this.initializeRun(extraParams || {}, metadata || {});
        this.prompt = prompts.map((p) => ({ role: 'user', content: p }));
        this.startTime = performance.now();
    }
    async handleLLMEnd(output, runId, parentRunId, tags) {
        this.endTime = performance.now();
        this.latency = this.endTime - this.startTime;
        this.extractTokenInformation(output);
        this.extractOutput(output);
        this.addToTrace();
    }
    extractTokenInformation(output) {
        if (this.provider === 'OpenAI') {
            this.openaiTokenInformation(output);
        }
    }
    openaiTokenInformation(output) {
        if (output.llmOutput && 'tokenUsage' in output.llmOutput) {
            this.promptTokens = output.llmOutput?.['tokenUsage']?.promptTokens ?? 0;
            this.completionTokens = output.llmOutput?.['tokenUsage']?.completionTokens ?? 0;
            this.totalTokens = output.llmOutput?.['tokenUsage']?.totalTokens ?? 0;
        }
    }
    extractOutput(output) {
        const lastResponse = output?.generations?.at(-1)?.at(-1) ?? undefined;
        this.output += lastResponse?.text ?? '';
    }
    addToTrace() {
        let name = 'Chat Completion Model';
        if (this.provider && this.provider in PROVIDER_TO_STEP_NAME) {
            name = PROVIDER_TO_STEP_NAME[this.provider] ?? 'Chat Completion Model';
        }
        (0, tracer_1.addChatCompletionStepToTrace)({
            name: name,
            inputs: { prompt: this.prompt },
            output: this.output,
            latency: this.latency,
            tokens: this.totalTokens,
            promptTokens: this.promptTokens,
            completionTokens: this.completionTokens,
            model: this.model,
            modelParameters: this.modelParameters,
            metadata: this.metadata,
            provider: this.provider ?? '',
        });
    }
}
exports.OpenlayerHandler = OpenlayerHandler;
//# sourceMappingURL=langchainCallback.js.map