import { BaseCallbackHandler } from '@langchain/core/callbacks/base';
import { LLMResult } from '@langchain/core/dist/outputs';
import type { Serialized } from '@langchain/core/load/serializable';
import { BaseMessage } from '@langchain/core/messages';
export declare class OpenlayerHandler extends BaseCallbackHandler {
    name: string;
    startTime: number | null;
    endTime: number | null;
    prompt: Array<{
        role: string;
        content: string;
    }> | null;
    latency: number;
    provider: string | undefined;
    model: string | null;
    modelParameters: Record<string, any> | null;
    promptTokens: number | null;
    completionTokens: number | null;
    totalTokens: number | null;
    output: string;
    metadata: Record<string, any>;
    constructor(kwargs?: Record<string, any>);
    handleChatModelStart(llm: Serialized, messages: BaseMessage[][], runId: string, parentRunId?: string | undefined, extraParams?: Record<string, unknown> | undefined, tags?: string[] | undefined, metadata?: Record<string, unknown> | undefined, name?: string): Promise<void>;
    private initializeRun;
    private langchainMassagesToPrompt;
    handleLLMStart(llm: Serialized, prompts: string[], runId: string, parentRunId?: string, extraParams?: Record<string, unknown>, tags?: string[], metadata?: Record<string, unknown>, runName?: string): Promise<void>;
    handleLLMEnd(output: LLMResult, runId: string, parentRunId?: string, tags?: string[]): Promise<void>;
    private extractTokenInformation;
    private openaiTokenInformation;
    private extractOutput;
    private addToTrace;
}
//# sourceMappingURL=langchainCallback.d.ts.map