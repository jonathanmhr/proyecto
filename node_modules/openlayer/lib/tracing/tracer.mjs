// tracing/tracer.ts
import { Trace } from "./traces.mjs";
import { UserCallStep, ChatCompletionStep } from "./steps.mjs";
import { StepType } from "./enums.mjs";
import Openlayer from "../../index.mjs";
let currentTrace = null;
const publish = process.env['OPENLAYER_DISABLE_PUBLISH'] != 'true';
let client = null;
if (publish) {
    console.debug('Publishing is enabled');
    client = new Openlayer();
}
function getCurrentTrace() {
    return currentTrace;
}
function setCurrentTrace(trace) {
    currentTrace = trace;
}
// Function to create a new step
const stepStack = [];
function createStep(name, stepType = StepType.USER_CALL, inputs, output, metadata = null) {
    metadata = metadata || {};
    let newStep;
    if (stepType === StepType.CHAT_COMPLETION) {
        newStep = new ChatCompletionStep(name, inputs, output, metadata);
    }
    else {
        newStep = new UserCallStep(name, inputs, output, metadata);
    }
    newStep.startTime = performance.now();
    const parentStep = getCurrentStep();
    const isRootStep = parentStep === null;
    if (isRootStep) {
        console.debug('Starting a new trace...');
        console.debug(`Adding step ${name} as the root step`);
        const currentTrace = new Trace();
        setCurrentTrace(currentTrace);
        currentTrace.addStep(newStep);
    }
    else {
        console.debug(`Adding step ${name} as a nested step to ${parentStep.name}`);
        currentTrace = getCurrentTrace();
        parentStep.addNestedStep(newStep);
    }
    stepStack.push(newStep);
    const endStep = () => {
        newStep.endTime = performance.now();
        newStep.latency = newStep.endTime - newStep.startTime;
        stepStack.pop(); // Remove the current step from the stack
        if (isRootStep) {
            console.debug('Ending the trace...');
            const traceData = getCurrentTrace();
            // Post process trace and get the input variable names
            const { traceData: processedTraceData, inputVariableNames } = postProcessTrace(traceData);
            if (publish && process.env['OPENLAYER_INFERENCE_PIPELINE_ID']) {
                client.inferencePipelines.data.stream(process.env['OPENLAYER_INFERENCE_PIPELINE_ID'], {
                    config: {
                        outputColumnName: 'output',
                        inputVariableNames: inputVariableNames,
                        groundTruthColumnName: 'groundTruth',
                        latencyColumnName: 'latency',
                        costColumnName: 'cost',
                        timestampColumnName: 'inferenceTimestamp',
                        inferenceIdColumnName: 'inferenceId',
                        numOfTokenColumnName: 'tokens',
                    },
                    rows: [processedTraceData],
                });
            }
            // Reset the entire trace state
            setCurrentTrace(null);
            stepStack.length = 0; // Clear the step stack
        }
        else {
            console.debug(`Ending step ${name}`);
        }
    };
    return [newStep, endStep];
}
function getCurrentStep() {
    const currentStep = stepStack.length > 0 ? stepStack[stepStack.length - 1] : null;
    return currentStep;
}
function getParamNames(func) {
    const STRIP_COMMENTS = /((\/\/.*$)|(\/\*[\s\S]*?\*\/))/gm;
    const ARGUMENT_NAMES = /([^\s,]+)/g;
    const fnStr = func.toString().replace(STRIP_COMMENTS, '');
    const result = fnStr.slice(fnStr.indexOf('(') + 1, fnStr.indexOf(')')).match(ARGUMENT_NAMES);
    return result || [];
}
// Higher-order function to trace synchronous or asynchronous functions
function trace(fn, stepType = StepType.USER_CALL, stepName) {
    return async function (...args) {
        const name = stepName || fn.name;
        const paramNames = getParamNames(fn);
        const inputs = Object.fromEntries(paramNames.map((name, index) => [name, args[index]]));
        const [step, endStep] = createStep(name, stepType, args);
        try {
            const result = await fn(...args);
            step.log({ inputs, output: result });
            return result;
        }
        catch (error) {
            step.log({ inputs, metadata: { error: error.message } });
            throw error;
        }
        finally {
            endStep();
        }
    };
}
export function addChatCompletionStepToTrace({ name, inputs, output, latency, tokens = null, promptTokens = null, completionTokens = null, model = null, modelParameters = null, rawOutput = null, metadata = {}, provider = 'OpenAI', }) {
    const [step, endStep] = createStep(name, StepType.CHAT_COMPLETION, inputs, output, metadata);
    if (step instanceof ChatCompletionStep) {
        step.provider = provider;
        step.promptTokens = promptTokens;
        step.completionTokens = completionTokens;
        step.tokens = tokens;
        step.model = model;
        step.modelParameters = modelParameters;
        step.rawOutput = rawOutput;
        step.latency = latency;
    }
    step.log({ inputs, output, metadata });
    endStep();
}
function postProcessTrace(traceObj) {
    const rootStep = traceObj.steps[0];
    const input_variables = rootStep.inputs;
    const inputVariableNames = input_variables ? Object.keys(input_variables) : [];
    const processed_steps = traceObj.toJSON();
    const traceData = {
        inferenceTimestamp: Math.floor(Date.now() / 1000),
        inferenceId: rootStep.id.toString(),
        output: rootStep.output,
        groundTruth: rootStep.groundTruth,
        latency: rootStep.latency,
        cost: 0,
        tokens: 0,
        steps: processed_steps,
    };
    if (input_variables) {
        Object.assign(traceData, input_variables);
    }
    return { traceData, inputVariableNames };
}
export default trace;
//# sourceMappingURL=tracer.mjs.map